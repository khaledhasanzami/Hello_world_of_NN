{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use MNIST dataset for classifying handwritten digits. MNIST dataset is considered the 'Hello World' of deep learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MNIST dataset, there are training and testing images with labels on them. Training images will be used as an input to the neural network and testing set will be used to measure the accuracy of the model. We are going to load the images to **images** and their corrsesponding values to **labels**. Here the labels specifies what digit is written on a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the shape of the imgaes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that, we have 60000 images and every image is 28$*$28 pixels, means each images has a height of 28 pixels and width of 28 pixels. As we can see this is a 3D tensor and the image shape is 2D. But neurons in the neural network recieves 1D values. Let's flatten or reshape the images. Flatten means making the 2D image to 1D vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the shape again, we can see this as a tuple of (60000,784). Here the 2D image were converted into 1D vector of 28$*$28=784 shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let;s look at test images also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see test images are also 2D. Let's make it 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see datatype is uint8(unsigned integer(0-255)2^8) which is discrete. The whole math for neural network is continuous not discrete. This is best approximated by floating point numbers. For compatibility, let's convert the the images to float32 and scale them between 0 and 1. Scaling increases accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the labels in categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's creat a neural network with two layer only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, **Sequential** means we are having a stack of layers. \n",
    "\n",
    "**Dense** means we are having densly connected neurons which means we are connecting evry input to every othe nueron or with more than one layer case, every neuron to other neuron. \n",
    "\n",
    "We are defining 1D vector of the images (784,).\n",
    "For clarification,\n",
    "  - 0D tensors shape = ()\n",
    "  - 1D tensors shape = (m,)\n",
    "  - 2D tensors shape = (m,n)\n",
    "  - 3D tensors shape = (x,m,n)\n",
    "\n",
    "We are having 10 way layer means it will return 10 probability scores.\n",
    "\n",
    "We used activation function as sigmoid function which is mathematically $\\frac{1}{1+e^{-z}}$. Sigmoid function converts any value between 0 and 1. Activation function is just describes if a neuron fires or not. If a neuron fires or activated the condition is fullfilled if not then the neuron will not fire.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always need to compile a model after bulding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "loss='categorical_crossentropy',\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,\n",
    " - **loss function** will be able to measure its performance on the training data and veer itself in the right way. [All the Loss functions](https://keras.io/api/losses/)\n",
    " - **optimizer** way to update the netowrk based on loss. [All the optimizers](https://keras.io/api/optimizers/)\n",
    " - **metrics**, we only cared about accuracy. Fraction of the images that will be correclty classified. [All the metrics](https://keras.io/api/metrics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2573 - accuracy: 0.9246\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1028 - accuracy: 0.9698\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0681 - accuracy: 0.9794\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0503 - accuracy: 0.9845\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0378 - accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26109553b70>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, epoch means how many times we want the itirations. DL models do not process entire dataset hence breaks them into small batches and operate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much accuracy we obtanied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0709 - accuracy: 0.9792\n",
      "test_acc: 0.979200005531311\n"
     ]
    }
   ],
   "source": [
    " test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
